# -*- coding: utf-8 -*-
"""Copy of Day 32: Project 3 - Books Data Analysis (JSON).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-TD67T76kSKK65SH6JK5EYJdWChtyhsK

* JSON DATASET: https://raw.githubusercontent.com/ozlerhakan/mongodb-json-files/master/datasets/grades.json
"""

import numpy as np

import pandas as pd

import re

import matplotlib.pyplot as plt

import seaborn as sns

from bs4 import BeautifulSoup

import requests

url = "https://raw.githubusercontent.com/ozlerhakan/mongodb-json-files/master/datasets/grades.json"

req = requests.get(url)

req.status_code

soup = BeautifulSoup(req.content)

soup

df = pd.read_json(url, lines=True)

df

df.info()

df._id[0]['$oid']

for i in range(len(df)):
  df['_id'][i] = df['_id'][i]['$oid']

df.head()

df.scores[0][0]

Introduce 3 new columns -> exam_score, quiz_score, homework_score (Avg(1,2,3))
Final dataset -> _id, student_id, class_id, exam_score, quiz_score, homework_score

"""Project 3 - Book Data Analysis (JSON)

URL: https://raw.githubusercontent.com/ozlerhakan/mongodb-json-files/master/datasets/books.json
"""

Steps:

1. vConvert the JSON data into a DataFrame
2. Explore the data
3. Clean & Manipulate the data
4. Perform the analysis
5. Conclusions

df = pd.read_json('https://raw.githubusercontent.com/ozlerhakan/mongodb-json-files/master/datasets/books.json', lines=True)

"""* Data Exploration"""

df.head(2)

df.columns

len(df.columns)

df.shape

df.shape[0]

df.shape[1]

df.info()

df.isnull().sum()

"""* Data Cleaning"""

df.head(2)

df[df['isbn'].isnull()]

df.drop(["isbn"], axis=1, inplace=True)

df.head(1)

df.duplicated(subset='title').sum()

df[df.duplicated(subset='title')]

df[df['title'] == 'Android in Practice']

df[df['title'] == 'SQL Server MVP Deep Dives']['authors'][174]

df[df['title'] == 'SQL Server MVP Deep Dives']['authors'][176]

# Delete all the duplicate rows
df.drop_duplicates('title', inplace=True)

df.duplicated('title').sum()

df.reset_index(inplace=True)

df.head()

df.drop('index', axis=1, inplace=True)

df.head(1)

df.info()

# Show me the data where pageCount is 0
df[df['pageCount']==0]

df.describe()

np.mean(df.pageCount)

np.median(df.pageCount)

df['pageCount'] = df['pageCount'].replace(0,int(np.median(df.pageCount)))

df.describe()

df.info()

df.head(1)



df['publishedDate'][0]['$date'].split('-')[0]

df['publishedDate'][0]['$date']

pd.to_datetime(df['publishedDate'][0]['$date']).day

newdf = df.copy()

for i in range(len(newdf.publishedDate)):
  if newdf['publishedDate'].isnull():
    pass
  else:
    newdf['publishedDate'] = int(newdf['publishedDate'][i]['$date'].split('-')[0])

df.info()

# Delete the columns - thumbnailUrl, shortDescription, longDescription

df.drop(['thumbnailUrl', 'shortDescription', 'longDescription'], axis=1, inplace=True)

df.info()

df.status

df.status.unique()

df.status.nunique()

df.status.value_counts()

# MEAP -> UNPUBLISH
df["status"] = df['status'].replace("MEAP","UNPUBLISH")

df.status.value_counts()

# PUBLISH -> 0
# UNPUBLISH -> 1
df["status"] = df['status'].replace("PUBLISH",0)
df["status"] = df['status'].replace("UNPUBLISH",1)

df.status.unique()

df.status.value_counts()

df.head()

# Show all the books (Name) where the author is W. Frank Ableson
z

for i in range(len(df)):
  if 'W. Frank Ableson' in df['authors'][i]:
    print(df['title'][i])

# List out the Top 10 books that has the highest number of Pages
df.nlargest(10, 'pageCount')[['title', 'pageCount']]